# MFA configuration for MNIST
defaults:
  - mfa
  - _self_

latent_dim: 10
n_clusters: 150
init_scale: 0.01
min_var: 0.01

trainer:
  lr: 2e-5
  n_epochs: 100
  # batch_size: 512  # commented out = full dataset E-step (approximate EM)
  batch_steps: 1000
  grad_clip: 0.0
  log_freq: 10

  # Regularization
  l1_reg: 0
  l2_reg: 0
  upr_prs_reg: 0
  lwr_prs_reg: 0
  mixture_entropy_reg: 0

  min_prob: 1e-5
  obs_min_var: 1e-6  # gans-n-gmms clips sqrt(D) to [1e-3, 1.0], so var in [1e-6, 1.0]
  obs_max_var: 1.0   # matches reference sqrt_D ≤ 1.0 → var ≤ 1.0
  lat_min_var: 1e-6

  # Jitter (optional, for additional stability)
  obs_jitter_var: 0.0
  lat_jitter_var: 0.0

analyses:
  generative_samples:
    enabled: true
    n_samples: 100
  cluster_statistics:
    enabled: true
  co_assignment_hierarchy:
    enabled: true
  co_assignment_merge:
    enabled: true
    filter_empty_clusters: true
    min_cluster_size: 0.0005
  optimal_merge:
    enabled: false
    filter_empty_clusters: true
    min_cluster_size: 0.0005
