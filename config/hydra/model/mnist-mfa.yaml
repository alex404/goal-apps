# MFA configuration for MNIST
# Hyperparameters based on Richardson & Weiss (2018) "GANs and GMMs"
defaults:
  - mfa
  - _self_

latent_dim: 10
n_clusters: 150
init_scale: 0.01
min_var: 0.01

trainer:
  lr: 2e-5
  n_epochs: 200
  batch_size: 512
  # batch_steps: 1000
  grad_clip: 0.0

  # Regularization - gans-n-gmms uses none
  l1_reg: 0
  l2_reg: 0
  upr_prs_reg: 0
  lwr_prs_reg: 0

  min_prob: 1e-3
  obs_min_var: 1e-6  # gans-n-gmms clips sqrt(D) to [1e-3, 1.0], so var in [1e-6, 1.0]
  lat_min_var: 1e-6

  # Jitter (optional, for additional stability)
  obs_jitter_var: 0.0
  lat_jitter_var: 0.0

analyses:
  generative_samples:
    enabled: true
    n_samples: 100
  cluster_statistics:
    enabled: true
  co_assignment_hierarchy:
    enabled: true
  co_assignment_merge:
    enabled: true
  optimal_merge:
    enabled: false
