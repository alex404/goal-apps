# MFA configuration for MNIST
defaults:
  - mfa
  - _self_

latent_dim: 10
n_clusters: 150
init_scale: 0.01
min_var: 0.01

trainer:
  lr: 2e-5           # matches reference (was 1e-4)
  n_epochs: 51       # ~6000 gradient steps with batch_size=512 (matches reference budget)
  batch_size: 512    # matches reference
  batch_steps: 1     # one step per batch (was 1000 — caused catastrophic collapse)
  epoch_reset: false # persist optimizer state across epochs (matches reference)
  grad_clip: 0.0     # no gradient clipping (matches reference)
  use_adamw: false   # plain Adam (matches reference optax.adam)

  # Regularization
  l1_reg: 0
  l2_reg: 0
  upr_prs_reg: 0
  lwr_prs_reg: 0
  mixture_entropy_reg: 0

  min_prob: 1e-5
  obs_min_var: 1e-6  # gans-n-gmms clips sqrt(D) to [1e-3, 1.0], so var in [1e-6, 1.0]
  obs_max_var: 1.0   # matches reference sqrt_D ≤ 1.0 → var ≤ 1.0
  lat_min_var: 1e-6

  # Jitter (optional, for additional stability)
  obs_jitter_var: 0.0
  lat_jitter_var: 0.0

analyses:
  generative_samples:
    enabled: true
    n_samples: 100
  cluster_statistics:
    enabled: true
  co_assignment_hierarchy:
    enabled: true
  co_assignment_merge:
    enabled: true
    filter_empty_clusters: true
    min_cluster_size: 0.0005
  optimal_merge:
    enabled: false
    filter_empty_clusters: true
    min_cluster_size: 0.0005
